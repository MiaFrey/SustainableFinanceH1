# -*- coding: utf-8 -*-
"""SustainableFinance_HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O1RUspAiA6o29Iug5BGz7_n8JSFSDsYJ

## **Data Cleaning**

### 1/ Link g-collab with your drive & Import libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy                    as np
import pandas                   as pd
import math
import openpyxl
import matplotlib.pyplot        as plt
import matplotlib.dates
import datetime
import seaborn                  as sns
from pypfopt                    import risk_models
from dateutil.relativedelta     import relativedelta
from pypfopt.expected_returns   import mean_historical_return
from pypfopt.risk_models        import sample_cov
from pypfopt.risk_models        import CovarianceShrinkage
from pypfopt.efficient_frontier import EfficientFrontier
from pypfopt                    import plotting
from tabulate                   import tabulate

"""### 2/ If you want to clean your data, run this part
You need to do this a least once to get some new saved excel files with cleaned data at /content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)
"""

### Finding a list of ISIN which corresponds to the companies we want to study

# Create datasets with the regions and scopes
data_region = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/region.xlsx')

data_scope1 = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/scope1.xlsx')
data_scope2 = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/scope2.xlsx')
data_scope3 = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/scope3.xlsx')

# Merge the databases each with the regions
df_merge  = pd.merge(data_scope1, data_region, on = 'ISIN')
df_merge2 = pd.merge(data_scope2, data_region, on = 'ISIN')
df_merge3 = pd.merge(data_scope3, data_region, on = 'ISIN')

# Keep only European firms
df_merge  = df_merge.drop(df_merge[df_merge.Country !='EUR'].index)
df_merge2 = df_merge2.drop(df_merge2[df_merge2.Country !='EUR'].index)
df_merge3 = df_merge3.drop(df_merge3[df_merge3.Country !='EUR'].index)

# To check later on if the scope 1,2,3 have the same firms
companies_names_scope1 = df_merge.ISIN.unique()
companies_names_scope2 = df_merge2.ISIN.unique()
companies_names_scope3 = df_merge3.ISIN.unique()

# Check if the companies we have in the scope 1, 2 and 3 are the same
if (companies_names_scope2.all() == companies_names_scope1.all()) and (companies_names_scope2.all() == companies_names_scope3.all()):
  print("The companies are the same")
else:
  print("NO GO")

# A df of 1 columns with all the ISIN (& Names --> need to take 0:2 for this) of the companies we are interested in
my_companies  = df_merge.iloc[: , 0:1]

### Testing the data cleaning (Jasmina used for report)
data_merged = pd.merge(data_scope1, data_scope2, how = 'inner', on = 'ISIN')
data_merged = pd.merge(data_merged, data_scope3, how = 'inner', on = 'ISIN')
data_merged = pd.merge(data_merged, data_region, on = 'ISIN')
df_merge4   = data_merged.drop(data_merged[data_merged.Country !='EUR'].index)

print(len(df_merge4))

if (companies_names_scope1.all() == df_merge4.ISIN.unique().all()):
  print("Succes!")

### Import datasets
data_div    = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/dy.xlsx')
data_prices = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/prices.xlsx')
data_size   = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/Data_Excel/Trucost_CO2emissions/size.xlsx')

### Clean datasets and transpose them

# Keep only entreprises we are interested in
my_dividends = pd.merge(my_companies, data_div, on=["ISIN"])
my_prices    = pd.merge(my_companies, data_prices, on=["ISIN"])
my_size      = pd.merge(my_companies, data_size, on=["ISIN"])

my_dividends_T         = my_dividends.drop(["ISIN","NAME"], axis=1).transpose()
my_dividends_T.columns = my_dividends.ISIN

my_prices_T            = my_prices.drop(["ISIN","NAME"], axis=1).transpose()
my_prices_T.columns    = my_prices.ISIN

my_size_T              = my_size.drop(["ISIN","NAME"], axis=1).transpose()
my_size_T.columns      = my_size.ISIN


# Create the file with returns
my_returns_T = my_prices_T.pct_change()
my_returns_T.replace([np.inf, -np.inf], np.nan, inplace = True)

# Create a list of corresponding dates
date      = datetime.date(1999, 12, 31)
row_name  = date.strftime("%m/%Y")
new_index = []
for i in range(len(my_size_T.index)):
  new_index.append(date)
  date = date + relativedelta(months=+1)
  #row_name = date.strftime("%m/%Y")

my_prices_T.index    = new_index
my_size_T.index      = new_index
my_returns_T.index   = new_index

# Drop year 1999
my_dividends_T = my_dividends_T.drop("x1999")
my_prices_T    = my_prices_T.drop(datetime.date(1999, 12, 31))
my_returns_T   = my_returns_T.drop(datetime.date(1999, 12, 31))

# Save files
file_name = '/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_dividends_T.xlsx'
my_dividends_T.to_excel(file_name, index=True)

file_name = '/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_prices_T.xlsx'
my_prices_T.to_excel(file_name, index=True)

file_name = '/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_size_T.xlsx'
my_size_T.to_excel(file_name, index=True)

file_name = '/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_returns_T.xlsx'
my_returns_T.to_excel(file_name, index=True)

"""### 3/ If you have already cleaned you data once, only run this part to import"""

### Import clean data (Don't delete)
my_dividends_T       = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_dividends_T.xlsx',index_col=0)
my_prices_T          = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_prices_T.xlsx'   ,index_col=0)
my_size_T_VW         = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_size_T.xlsx'     ,index_col=0)
my_size_T            = my_size_T_VW.drop(datetime.date(1999, 12, 31))
my_returns_T         = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_returns_T.xlsx'  ,index_col=0)
my_rf                = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/FactorsAndRiskFreeRates/eurf.xlsx', header=None, names = ["Date", "rf"])
my_EU_market_returns = pd.read_excel(r'/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/FactorsAndRiskFreeRates/eumarket.xlsx', header=None, names = ["Date", "returns"])

"""## **Exercice 1**"""

### Calculation of the returns
return_Q1  = my_returns_T.copy()
mean_Q1    = return_Q1.mean()*12
vol_Q1     = return_Q1.std()*(math.sqrt(12))
print("The correlation between the average annualized return and the volatility of the securities is: ", mean_Q1.corr(vol_Q1))
# We see that this is in the right direction (with respect to the theory), but we can perhaps find a better result if we temporarily discard the NaNs.
return_Q1b = return_Q1.dropna(axis=1) # this makes the number of companies drop from 2739 to 1371 (drop any column with a NaN value (post (=after) dec 1999 line dropped above))
mean_Q1b   = return_Q1b.mean()*12
vol_Q1b    = return_Q1b.std()*(math.sqrt(12))
print("The correlation is now: ", mean_Q1b.corr(vol_Q1b), "We can this see that this improves the observed correlation between securities, even though the basket of assets was reduced from 2739 to 1371 (less than half). This is not ideal, so we will not consider the reduced sample for upcoming exercises, but we will keep in mind the NaN and adapt our code to best reflect the reality behind the data.")

### Compute the correlation btw the average annualized returns and the annualized volatility

my_returns_add = return_Q1b.copy() # used to add further data for this exercise
# Calculate the monthy average (mean per column)
my_monthly_average_return = my_returns_add.mean()
# Append this to the end of our dataframe (NB: we lose the indexing names)(line at position 252)
my_returns_add = my_returns_add.append(my_monthly_average_return,ignore_index=True) # Now the last row (253th row denoted by position 252) contains the column average (axis=0)

# Calculate the annualized average return (based on the monthly average)
my_annualized_average_return = my_returns_add.iloc[-1]*12
# Append this to the end of our dataframe (line at position 253)
my_returns_add = my_returns_add.append(my_annualized_average_return,ignore_index=True) # Now we have 254 rows (last of which is the annualized average return!)

# Compute and report the monthly volatility for all individual assets
my_monthly_volatility = my_returns_add.iloc[0:252,:].std()
# Append to the end of df (line at position 254)
my_returns_add = my_returns_add.append(my_monthly_volatility,ignore_index=True)

# Calculate the annualized volatility and save it to the DataFrame "my_returns"
my_annualized_volatility = my_returns_add.iloc[-1]*math.sqrt(12)
# Append to the end of df (line at position 255)
my_returns_add = my_returns_add.append(my_annualized_volatility,ignore_index=True)

# Compute the correlation between individual average returns and volatility
risk_return_corr = my_returns_add.iloc[255,:].corr(my_returns_add.iloc[253,:])
print("The correlation found between the average annualized returns and the annualized volatility across securities is: ", risk_return_corr)

### Summary Statistics for Q1

# Create data
data = [["Monthly average return", my_monthly_average_return.values], # We must use .values to extract only the values from a pd.Series
        ["Annualized average return", my_annualized_average_return.values],
        ["The Average of all Annualized average return", my_annualized_average_return.mean()], 
        ["Monthly volatility", my_monthly_volatility.values], 
        ["Annualized volatility", my_annualized_volatility.values],
        ["The Average of all Annualized volatilities", my_annualized_volatility.mean()], 
        ["Correlation risk-return", risk_return_corr]]
  
# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Question 1: Summary Statistics",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

### Graph creation

plt.scatter(my_annualized_average_return.index,my_annualized_average_return)
plt.title("Annualized Average Return Per Company")
plt.xlabel("Companies")
plt.ylabel("Returns")
plt.tick_params(
   axis='x',          # Changes apply to the x-axis
   which='both',      # Both major and minor ticks are affected
   bottom=False,      # Ticks along the bottom edge are off
   top=False,         # Ticks along the top edge are off
   labelbottom=False) # Labels along the bottom edge are off
plt.show()
fn = '/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_annualized_average_return_plot.png'
plt.savefig(fn)
print("The mean of the annualized average return per company is: ", my_annualized_average_return.mean(),".")

### Graph creation my_annualized_volatility.values

plt.scatter(my_annualized_volatility.index, my_annualized_volatility)
plt.title("Annualized Average Volatility Per Company")
plt.xlabel("Companies")
plt.ylabel("Volatility")
plt.tick_params(
   axis='x',          # Changes apply to the x-axis
   which='both',      # Both major and minor ticks are affected
   bottom=False,      # Ticks along the bottom edge are off
   top=False,         # Ticks along the top edge are off
   labelbottom=False) # Labels along the bottom edge are off
plt.show()
fn = '/content/drive/MyDrive/Sustainable Finance (EU-Scope1-3)/my_annualized_average_return_plot.png'
plt.savefig(fn)
print("The mean of the annualized average return per company is: ", my_annualized_volatility.mean(),".")

# Here it's most interesting to take all the european firms (even though there 
#   are some with NaN values, because with 2739 firms, we have more differences 
#   and we can observe some negative correlations --> better opportunity 
#   for diversification!)
ind_asset_return_corr = return_Q1.corr(method='pearson')

ind_asset_return_corr.style.background_gradient(cmap='coolwarm')

# Correlation heatmap of all the firms
sns.heatmap(ind_asset_return_corr)

### Correlation matrix of a sample
r2 = return_Q1b.sample(n=10,axis=1).reset_index().drop("index", axis=1)
# Return_Q1b.iloc[:,300:310] 
cov_matrix = r2.cov()
# Print(cov_matrix)

plotting.plot_covariance(cov_matrix, plot_correlation=True);

"""## **Exercice 2**"""

### Calculate the equally weighted portfolio

return_Q2a = my_returns_T.copy()

# Initialize the list
eq_portf_returns = ['eq. w. portf','']

# For each month
for i in range(len(return_Q2a.index)):
  sum = 0
  # Create a df of only the values of interest, clean it and calculate the 
  # weight (equal for) each return
  monthly_return_Q2a = return_Q2a.iloc[i,:]
  monthly_return_Q2a =  monthly_return_Q2a.dropna()
  weight             = 1/len(monthly_return_Q2a)

  for n in range(len(monthly_return_Q2a)):
    sum += (monthly_return_Q2a.iloc[n]*weight)
  
  eq_portf_returns.append(sum)

### Calculate the performance of the portfolio

# Create a dataframe
pf_EW = pd.DataFrame(eq_portf_returns[2:], columns = ['eq_portf_returns'])
  
# Annualized average return = (AAR)
AAR_EW_Q2 = pf_EW.mean()*12

# Annualized volatility
AV_EW_Q2 = pf_EW.std()*math.sqrt(12)
  
# Minimum return of portfolio (of the values of the loop above)
Min_ret_EW_Q2 = pf_EW.min()

# Maximum return
Max_ret_EW_Q2 = pf_EW.max()
 
# Sharpe ratio
SR_EW = (AAR_EW_Q2 - my_rf["rf"].mean()*12) / AV_EW_Q2

# Plot the timeseries
pf_EW['date'] = pd.date_range(start='31/1/2000', periods = len(pf_EW), freq = 'M')
pf_EW.plot(x = "date", y = "eq_portf_returns")

### Make a table with Summary Statistics

# Create data
data = [["Annualized average return", AAR_EW_Q2], 
        ["Annualized volatility", AV_EW_Q2], 
        ["Minimum return", Min_ret_EW_Q2], 
        ["Maximum return", Max_ret_EW_Q2],
        ["Sharpe ratio", SR_EW]]
  
# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Equally-weighted portfolio: Summary statistics",'\033[1m')
print(tabulate(data, headers = col_names, tablefmt = "fancy_grid"))

### Calculate the value-weighted portfolio with monthly rebalancing

return_Q2b = my_returns_T.copy()
size_Q2b   = my_size_T_VW.copy()

# Initialize the list -> The goal is to append it to the returns.xlsx (or not :) )
mr_portf_returns = ['val. w. portf','']

# For each month
for i in range(len(return_Q2b.index)):
  sum = 0
  # Create a df of only the values of interest, clean it and calculate the 
  # total size of all the assets
  monthly_returns_Q2b = return_Q2b.iloc[i,:].dropna()
  monthly_size_Q2b    = size_Q2b.iloc[i,:]
  monthly_data_Q2b    = pd.concat([monthly_returns_Q2b, monthly_size_Q2b], axis=1).dropna() # We drop here because there are more NaNs in the size data set, which don't correspond to the NaNs in return(companies not traded), so we drop after the concat for calculation purposes.
  tot_size_Q2b        = monthly_data_Q2b.iloc[:,1].sum()

  for n in range(len(monthly_data_Q2b)):
    # Adapt the weight for each month: weight = size_asset_i/tot_size_of_assets
    weight = monthly_data_Q2b.iloc[n,1] / tot_size_Q2b
    sum += (monthly_data_Q2b.iloc[n,0] * weight)
  
  mr_portf_returns.append(sum)

### Calculate the performance of the portfolio

# Create a dataframe
pf_VW = pd.DataFrame(mr_portf_returns[2:], columns = ['mr_portf_returns'])
  
# Annualized average return = (AAR)
AAR_VW_Q2 = pf_VW.mean()*12

# Annualized volatility
AV_VW_Q2 = pf_VW.std()*math.sqrt(12)
  
# Minimum return of portfolio (of the values of the loop above)
Min_ret_VW_Q2 = pf_VW.min()

# Maximum return
Max_ret_VW_Q2 = pf_VW.max()

# Sharpe ratio
SR_VW = (AAR_VW_Q2 - my_rf["rf"].mean()*12) / AV_VW_Q2

# Plot the timeseries
pf_VW['date'] = pd.date_range(start='31/1/2000', periods = len(pf_VW), freq = 'M')
pf_VW.plot(x = "date", y = "mr_portf_returns")

### Make a table with Summary Statistics

# Create data
data = [["Annualized average return", AAR_VW_Q2], 
        ["Annualized volatility", AV_VW_Q2], 
        ["Minimum return", Min_ret_VW_Q2], 
        ["Maximum return", Max_ret_VW_Q2],
        ["Sharpe ratio", SR_VW]]
  
# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Value-weighted portfolio: Summary statistics",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

"""## **Exercice 3**"""

### Investing all in the asset with the highest annualized average return
##      We are finding and excluding outliers (2 of them) first

# Find the asset with the highest annualized average return computed in point 1
AAR_max = my_annualized_average_return.idxmax() # gets the ISIN of company which has 
                                                # the highest annualized return
# Show the annualized average return and annualized volatility of this one-asset portfolio
g = my_annualized_average_return[AAR_max]
h = my_annualized_volatility[AAR_max]
print('The name of the asset with the highest annualized average is', AAR_max,
      '. It is annualized average return is', g, 'and its annualized volatility is', h, '.') 

return_Q1b.copy()[AAR_max].plot() 
print('The maximum monthly return is', return_Q1b.copy()[AAR_max].max(), '.')

"""This is an outlier -> We decide to remove it


"""

# Find the asset with the highest annualized average return computed in point 1, droping 1st outlier
AAR_max2 = my_annualized_average_return.drop(AAR_max).idxmax() # gets the ISIN of the company 
                                                               # which has the highest annualized 
                                                               # return

# Show the annualized average return and annualized volatility of this one-asset portfolio
g2 = my_annualized_average_return[AAR_max2]
h2 = my_annualized_volatility[AAR_max2]
print('The name of the asset with the highest annualized average is', AAR_max2,'. It is annualized average return is', g2, 'and its annualized volatility is', h2, '.') 

return_Q1b.copy()[AAR_max2].plot() 
print('The maximum monthly return is', return_Q1b.copy()[AAR_max2].max(), '.')

"""This is also an outlier -> To be removed"""

# Find the asset with the highest annualized average return computed in point 1 dropping 2nd outlier
my_annualized_average_return3 = my_annualized_average_return.drop(AAR_max)
AAR_max3 = my_annualized_average_return3.drop(AAR_max2).idxmax() # gets the ISIN the 
                                                                 # company which has the highest 
                                                                 # annualized return

# Show the annualized average return and annualized volatility of this one-asset portfolio
g3 = my_annualized_average_return[AAR_max3]
h3 = my_annualized_volatility[AAR_max3]

return_Q1b.copy()[AAR_max3].plot() 


# Summary Statistics
data = [["Company", AAR_max3], 
        ["Highest annualized average return", g3], 
        ["Annualized volatility", h3], 
        ["Maximum monthly return", return_Q1b.copy()[AAR_max3].max()]]

# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Statistics of the firm with the highest annualized average return",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

### Investing all in the asset with the highest annualized average return over 2 years

# Annualized of 2 years
my_returns_2_years = return_Q1b.iloc[0:24,:]

# Calculate the annualized average return of the 2 first years
my_ann_avr_ret_2yr = my_returns_2_years.mean()
my_ann_avr_ret_2yr = my_ann_avr_ret_2yr*12

# Find the max
AAR_max3b = my_ann_avr_ret_2yr.idxmax()

# Calculate the annualized_volatility of the first two years
my_ann_volatility_2yr = my_returns_2_years.std()*(math.sqrt(12))

my_returns_2_years[AAR_max3b].plot()

# Summary Statistics
data = [["Company", AAR_max3b], 
        ["Highest annualized average return", my_ann_avr_ret_2yr[AAR_max3b]], 
        ["Annualized volatility", my_ann_volatility_2yr[AAR_max3b]]]

# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Statistics of the firm with the highest annualized ")
print("average return over 2 years",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

# Remove the two outliers found 
zz           = ["DE000A1MMCC8", "DE000A255G36"]
my_returns_T = my_returns_T.drop(zz, axis=1)
my_prices_T  = my_prices_T.drop(zz, axis=1)
my_size_T    = my_size_T.drop(zz, axis=1)
my_size_T_VW = my_size_T_VW.drop(zz, axis=1)

"""## **Exercice 4**"""

### Keeping only 50 random firms
returns_Q4     = my_returns_T.copy()
prices_Q4      = my_prices_T.copy()
random50       = returns_Q4.sample(n=50,axis=1).reset_index().drop("index", axis=1)
random50.index = returns_Q4.index

### Optimal portfolio with minimum variance and monthly rebalancing
##      Using a rolling window of 5 years

# Definitions
FIVE_YEARS = 60 # 5yr * 12months = 60 months
RETURN     = 0

rand50_Q4 = random50.copy()

# Initialize the list
MV_pf_returns = []

# For each month
for m in range(FIVE_YEARS, len(rand50_Q4)):
  monthly_returns = rand50_Q4.iloc[m,:]

  # Drop the firms which are not selling shares
  monthly_returns = monthly_returns.dropna()
  prices_Q4_r     = prices_Q4[monthly_returns.index]

  # Rolling window on 5 years
  prices_Q4_r = prices_Q4_r.iloc[m-FIVE_YEARS:m,:]

  # Portfolio Creation
  S       = CovarianceShrinkage(prices_Q4_r, frequency=12).ledoit_wolf()
  ef      = EfficientFrontier(monthly_returns.values.tolist(), S) 
  weights = ef.min_volatility()

  MV_pf_returns.append(ef.portfolio_performance()[RETURN])

### Calculation of the portfolio properties

# Create a dataframe
pf_MV = pd.DataFrame(MV_pf_returns, columns = ['MV_pf_returns'])

# Annualized average return = (AAR)
AAR_MV_Q4 = pf_MV.mean()*12

# Annualized volatility
AV_MV_Q4 = pf_MV.std()*math.sqrt(12)
  
# Minimum return of portfolio (of the values of the loop above)
Min_ret_MV_Q4 = pf_MV.min()

# Maximum return
Max_ret_MV_Q4 = pf_MV.max()

# Sharpe ratio
SR_MV = (AAR_MV_Q4 - my_rf["rf"].mean()*12) / AV_MV_Q4

### Summary Statistics to report the results

# Create data
data = [["Annualized average return", AAR_MV_Q4], 
        ["Annualized volatility", AV_MV_Q4], 
        ["Minimum return", Min_ret_MV_Q4], 
        ["Maximum return", Max_ret_MV_Q4],
        ["Sharpe ratio", SR_MV]]
  
# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Minimum variance portfolio with monthly rebalancing: Summary statistics",'\033[1m')
print(tabulate(data, headers = col_names, tablefmt = "fancy_grid"))

### Plots

pf_MV['date'] = pd.date_range(start='31/1/2000', periods=len(pf_MV), freq='M')
pf_MV.plot(x = "date", y="MV_pf_returns")

"""## **Exercice 5**"""

### Optimal portfolios with various target portfolio returns

# Definitions
TARGETED_RETURN_START = 0.02
NB_OF_PORTFOLIOS      = 8
SHARPE_RATIO          = 2

targeted_return = TARGETED_RETURN_START

# Get the correct df for the exercices
rand50_Q5 = random50.copy()
prices_Q5 = my_prices_T.copy()
prices_Q5 = prices_Q5[rand50_Q5.columns]

# Average returns will be used to create the portfolio
avr_returns = rand50_Q5.mean(axis=0)

# To save the results
tr_portf_sharpe = np.zeros((NB_OF_PORTFOLIOS,2)) # Create array with zeros to change later on

# Create a plot of the portfolio
S  = CovarianceShrinkage(prices_Q5, frequency = 12).ledoit_wolf()
ef = EfficientFrontier(avr_returns.values, S)
plotting.plot_efficient_frontier(ef)

# Calculate the steps used to create NB_OF_PORTFOLIOS. 
#     Aiming for a return of 16% was not feasible so instead we create the 
#     same number of portfolio but in a range which corresponds to our data 

# Calculates the step to still have 8 portfolios but between 0.2 and 
# max existing, round 3 after . (choice)
return_step = round((ef._max_return_value - TARGETED_RETURN_START)/NB_OF_PORTFOLIOS,3)


for r in range(NB_OF_PORTFOLIOS):
  weights              = ef.efficient_return(target_return = targeted_return)
  cleaned_weights      = ef.clean_weights()
  tr_portf_sharpe[r,0] = targeted_return
  tr_portf_sharpe[r,1] = ef.portfolio_performance()[SHARPE_RATIO]
  targeted_return      = targeted_return + return_step

## Creation of the table
# Define header names
col_names = ["Targeted Return", "Sharpe Ratio"]
  
# Display table
print("Sharpe ratio per targeted return for the 50 randomly selected firms",'\033[1m')
print(tabulate(tr_portf_sharpe, headers=col_names, tablefmt ="fancy_grid"))

"""## **Exercice 6**"""

### Compare the performance of our portfolios with the EU benchmark

#Definition
TO_2005 = 62     # 5yr * 12months + 2 titres
FROM_2005 = 36   #3yr * 12months

EU_returns_Q8 = my_EU_market_returns    # /!\ EU market returns start only from the 2002-01-31
EW_pf_returns = eq_portf_returns.copy() #equally weighted portfolio starting from 2000
VW_pf_returns = mr_portf_returns.copy() #equally weighted (monthly rebalancing portfolio starting from 2000
min_var_ret   = MV_pf_returns.copy()    # Minimun variance pf -> Sorry le nom de var était déja pris...


dates = EU_returns_Q8.Date
dates = dates[FROM_2005:]

returns_EU = EU_returns_Q8.returns + my_rf.iloc[:,1]
returns_EU = returns_EU[FROM_2005:]

del EW_pf_returns[:TO_2005] #drop months from jan 2000 to dec 2001
del VW_pf_returns[:TO_2005] #drop months from jan 2000 to dec 2001


plt.plot(dates, returns_EU,     label='EU market return')
plt.plot(dates, EW_pf_returns,  label='Equally weighted pf')
plt.plot(dates, VW_pf_returns,  label='Value weighted pf')
plt.plot(dates, min_var_ret,    label='Min variance pf')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Creating table for report
EWpd = pd.DataFrame(EW_pf_returns)
EUpd = pd.DataFrame(EU_returns_Q8.returns) 
VWpd = pd.DataFrame(VW_pf_returns)
MVpd = pd.DataFrame(min_var_ret)

#create data
data = [["Annualized average return", (EWpd.mean())*12, VWpd.mean()*12, MVpd.mean()*12, EUpd.mean()*12], 
        ["Annualized volatility", EWpd.std()*math.sqrt(12), VWpd.std()*math.sqrt(12), MVpd.std()*math.sqrt(12), EUpd.std()*math.sqrt(12)], 
        ["Minimum return", EWpd.min(), VWpd.min(), MVpd.min(), EUpd.min()], 
        ["Maximum return", EWpd.max(), VWpd.max(),MVpd.max(), EUpd.max()]]
  
#define header names
col_names = ["Summary Statistic", "Equally-Weighted", "Value-Weighted", "Minimum Variance", "EU Market"]
  
#display table
print("Performance Comparison Table",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

"""## **Exercice 7**"""

### Correlation between returns, volatility, size.


size_Q7 = my_size_T.mean() # The average size
return_Q7 = my_annualized_average_return.copy()
volatility_Q7 = my_annualized_volatility.copy()
# Compute and comment on the simple correlation between returns & volatility.
print(return_Q7.corr(volatility_Q7))

# Compute and comment on the simple correlation between returns & size.
print(return_Q7.corr(size_Q7))

# Compute and comment on the simple correlation between volatility & size.
volatility_Q7.corr(size_Q7)

"""## **Exercice 8**"""

###  Minimum variance portfolio with monthly rebalancing excluding smallest firms

prices_Q8  = my_prices_T.copy()
size_Q8 = my_size_T.copy()
rand50_Q8 = random50.copy()

size_Q8 = size_Q8[rand50_Q8.columns]

# Definitions 
START_KEEPING = round(len(rand50_Q8.columns)/3)
FIVE_YEARS    = 60

# Start new list
MV_pf_returns_Q8 = []


for r in range(FIVE_YEARS, len(rand50_Q8)):
  # Sort size where we only keep the two top tercile (the smallest firms are exclude)
  size_line_tosort = size_Q8.iloc[r,:]
  size_line_sorted = size_line_tosort.sort_values(axis=0, ascending=True, inplace=False, kind='quicksort', na_position='first', ignore_index=False, key=None)
  size_droped = size_line_sorted[START_KEEPING:]

  # Take the prices & returns of the firms we are interested in
  prices_Q8_r = prices_Q8[size_droped.index]
  rand50_Q8_r = rand50_Q8[size_droped.index]

  # Only the returns of the month we are interested in and clean it
  monthly_returns = rand50_Q8_r.iloc[r,:]
  monthly_returns = monthly_returns.dropna()

  # Only the prices of the month we are interested in
  prices_Q8_r = prices_Q8_r[monthly_returns.index]

  # Rolling window on 5 years
  prices_Q8_r = prices_Q8_r.iloc[r-FIVE_YEARS:r,:]

  # Portfolio Creation
  S = CovarianceShrinkage(prices_Q8_r, frequency=12).ledoit_wolf()
  ef = EfficientFrontier(monthly_returns.values.tolist(), S) 
  weights = ef.min_volatility()

  MV_pf_returns_Q8.append(ef.portfolio_performance()[0])

### Calculate the portfolio performance

# Create a dataframe
pf_MV_Q8 = pd.DataFrame(MV_pf_returns_Q8, columns = ['MV_pf_returns_Q8'])

# Annualized average return = (AAR)
AAR_MV_Q8 = pf_MV_Q8.mean()*12

# Annualized volatility
AV_MV_Q8 = pf_MV_Q8.std()*math.sqrt(12)
  
# Minimum return of portfolio (of the values of the loop above)
Min_ret_MV_Q8 = pf_MV_Q8.min()

# Maximum return
Max_ret_MV_Q8 = pf_MV_Q8.max()

# Sharpe ratio
SR_MV_Q8 = (AAR_MV_Q8 - my_rf["rf"].mean()*12) / AV_MV_Q8

### Summary Statistics

# Create data
data = [["Annualized average return", AAR_MV_Q8], 
        ["Annualized volatility", AV_MV_Q8], 
        ["Minimum return", Min_ret_MV_Q8], 
        ["Maximum return", Max_ret_MV_Q8],
        ["Sharpe ratio", SR_MV_Q8]]
  
# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("Minimum variance portfolio with monthly rebalancing: Summary statistics",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

pf_MV_Q8['date'] = pd.date_range(start='31/1/2000', periods=len(pf_MV_Q8), freq='M')
pf_MV_Q8.plot(x = "date", y="MV_pf_returns_Q8")

"""## **Exercice 9**"""

### Funciton to calculate the equally weighted portfolio

# Inputs: returns
# Outputs: average return of the pf

def fct_EW_creation(returns):
  sum = 0
  # Create a df of only the values of interest, clean it and calculate the 
  # weight (equal for) each return
  if len(returns)==0:
    weight = "nan"
  else:
    weight = 1/len(returns)

  for n in range(len(returns)):
    sum += (returns[n]*weight)
    
  return sum

### Funciton to calculate the value weighted portfolio

# Inputs: returns, size
# Outputs: average return of the pf

def fct_VW_creation(returns,size):
  sum = 0
  # Create a df of only the values of interest, clean it and calculate the 
  # weight (equal for) each return
  total_size = size.sum()

  for n in range(len(returns)):
    if np.isnan(size[n]):
      weight = 0
    else:
      weight = size[n]/total_size

    sum += (returns[n]*weight)
    
    
    
  return sum

### Equally-weighted and value-weighted portfolios for each size quantiles and long-short for Q1&Q5

prices_Q9  = my_prices_T.copy()
returns_Q9 = my_returns_T.copy()
size_Q9    = my_size_T.copy()
size_Q9_VW = my_size_T_VW.copy()

QUANTILE_1 = 0 # Small firms
QUANTILE_2 = 1
QUANTILE_3 = 2
QUANTILE_4 = 3
QUANTILE_5 = 4 # Big firms
NB_QUANTILES = 5

# Create a list for the return of the EW
avr_ret_EW    = []
avr_ret_VW    = []
avr_ret_EW_sl = []
avr_ret_VW_sl = []


for m in range(len(returns_Q9)):
  avr_ret_EWq = []
  avr_ret_VWq = []
  avr_ret_EW_sl_q = 0   # For the portfolio short&long
  avr_ret_VW_sl_q = 0   # For the portfolio short&long
  
  # Order size
  size_sorted = size_Q9.iloc[m,:].sort_values(axis=0, ascending=True, ignore_index=False).dropna()

  # split size in 5 quintiles -> Liste of names
  q_list = np.array_split(size_sorted,5) # Split the pd.Series into 5 equal parts

  for q in range(NB_QUANTILES):
    # Only the returns of the month & firms we are interested in and clean it
    return_q = returns_Q9[q_list[q].index]
    return_q = return_q.iloc[m,:].dropna()

    # Only the size of the month & firms we are interested in for the VW pf
    size_q = size_Q9_VW[return_q.index]
    size_q = size_q.iloc[m,:]

    # Only the prices of the month & firms we are interested in
    prices_q = prices_Q9[return_q.index]

    # Create an EW pf 
    avrR_EW = fct_EW_creation(return_q)
    avr_ret_EWq.append(avrR_EW)

    # Create a VW pf
    avrR_VW = fct_VW_creation(return_q, size_q)
    avr_ret_VWq.append(avrR_VW)

    # For the EW & VW long & short pf
    if q == QUANTILE_1:
      avrR_EW_q1 = avrR_EW
      avrR_VW_q1 = avrR_VW
    elif q == QUANTILE_5:
      avr_ret_EW_sl_q = ((0.5*avrR_EW_q1)-(0.5*avrR_EW))
      avr_ret_VW_sl_q = ((0.5*avrR_VW_q1)-(0.5*avrR_VW))

  # Add to the final list
  avr_ret_EW.append(avr_ret_EWq)
  avr_ret_VW.append(avr_ret_VWq)
  avr_ret_EW_sl.append(avr_ret_EW_sl_q)
  avr_ret_VW_sl.append(avr_ret_VW_sl_q)


print(avr_ret_EW)
print(avr_ret_VW)
print(avr_ret_EW_sl)
print(avr_ret_VW_sl)

### Graphs

# Eq. weighted
df = pd.DataFrame(avr_ret_EW)

plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_2], label='Quantile 2')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_3], label='Quantile 3')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_4], label='Quantile 4')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.title('Avr. return eq. weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Value weighted
df = pd.DataFrame(avr_ret_VW)

plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_2], label='Quantile 2')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_3], label='Quantile 3')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_4], label='Quantile 4')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.title('Avr. return value weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Short-long eq. w.
plt.plot(returns_Q9.index, avr_ret_EW_sl)
plt.title('Short-long eq. weighted')
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Short-long value w.
plt.plot(returns_Q9.index, avr_ret_VW_sl)
plt.title('Short-long value weighted')
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

import statistics
print(statistics.mean(avr_ret_EW_sl),"&",statistics.mean(avr_ret_VW_sl))

# report
df = pd.DataFrame(avr_ret_EW)

l1 = [df.iloc[:,QUANTILE_1].mean()]*len(returns_Q9)
l5 = [df.iloc[:,QUANTILE_5].mean()]*len(returns_Q9)

plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.plot(returns_Q9.index, l1, label='Q1: Avg. return across months')
plt.plot(returns_Q9.index, l5, label='Q5: Avg. return across months')
plt.title('Avg. return eq. weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

print("The average returns of Q1 over all years: ", df.iloc[:,QUANTILE_1].mean(), ". The average returns of Q5 over all years: ", df.iloc[:,QUANTILE_5].mean())
print(
    df.iloc[:,QUANTILE_1].mean(),
    df.iloc[:,QUANTILE_2].mean(),
    df.iloc[:,QUANTILE_3].mean(),
    df.iloc[:,QUANTILE_4].mean(),
    df.iloc[:,QUANTILE_5].mean()
)

df = pd.DataFrame(avr_ret_VW)

l1 = [df.iloc[:,QUANTILE_1].mean()]*len(returns_Q9)
l5 = [df.iloc[:,QUANTILE_5].mean()]*len(returns_Q9)

plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index, df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.plot(returns_Q9.index, l1, label='Q1: Avg. return across months')
plt.plot(returns_Q9.index, l5, label='Q5: Avg. return across months')
plt.title('Avg. return value weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

print("The average returns of Q1 over all years: ", df.iloc[:,QUANTILE_1].mean(), ". The average returns of Q5 over all years: ", df.iloc[:,QUANTILE_5].mean())
print(
    df.iloc[:,QUANTILE_1].mean(),
    df.iloc[:,QUANTILE_2].mean(),
    df.iloc[:,QUANTILE_3].mean(),
    df.iloc[:,QUANTILE_4].mean(),
    df.iloc[:,QUANTILE_5].mean()
)

### Equally-weighted and value-weighted portfolios for each past stock returns 
### quantiles and long-short for Q1&Q5

prices_Q9  = my_prices_T.copy()
returns_Q9 = my_returns_T.copy()
size_Q9    = my_size_T.copy()
size_Q9_VW = my_size_T_VW.copy()

# Definitions
STARTING_MONTH = 12
SUM_ON_MONTHS  = 12
QUANTILE_1     = 0 #Small firms
QUANTILE_2     = 1
QUANTILE_3     = 2
QUANTILE_4     = 3
QUANTILE_5     = 4 #Big firms
NB_QUANTILES   = 5

# Create a list for the return of the EW
avr_ret_EW    = []
avr_ret_VW    = []
avr_ret_EW_sl = []
avr_ret_VW_sl = []


for m in range(STARTING_MONTH, len(returns_Q9)):
  avr_ret_EWq = []
  avr_ret_VWq = []
  avr_ret_EW_sl_q = 0 # For the portfolio short&long
  avr_ret_VW_sl_q = 0 # For the portfolio short&long
  
  # Order size
  cum_ret = returns_Q9.iloc[m - SUM_ON_MONTHS:m,:].sum(axis=0)
  cum_ret_sorted = cum_ret.sort_values(axis=0, ascending=True, ignore_index=False).dropna()

  # split size in 5 quintiles -> Liste de nom
  q_list = np.array_split(cum_ret_sorted,5) # Split the pd.Series into 5 equal parts

  for q in range(NB_QUANTILES):
    # Only the returns of the month & firms we are interested in and clean it
    return_q = returns_Q9[q_list[q].index]
    return_q = return_q.iloc[m,:].dropna()

    size_q = size_Q9_VW[return_q.index]
    size_q = size_q.iloc[m,:]

    # Create an EW pf 
    avrR_EW = fct_EW_creation(return_q)
    avr_ret_EWq.append(avrR_EW)

    # Create a  VW pf
    avrR_VW = fct_VW_creation(return_q, size_q)
    avr_ret_VWq.append(avrR_VW)

    # For the EW & VW long & short pf
    if q == QUANTILE_1:
      avrR_EW_q1 = avrR_EW
      avrR_VW_q1 = avrR_VW
    elif q == QUANTILE_5:
      avr_ret_EW_sl_q = ((0.5*avrR_EW_q1)-(0.5*avrR_EW))
      avr_ret_VW_sl_q = ((0.5*avrR_VW_q1)-(0.5*avrR_VW))

  # Add to the final list
  avr_ret_EW.append(avr_ret_EWq)
  avr_ret_VW.append(avr_ret_VWq)
  avr_ret_EW_sl.append(avr_ret_EW_sl_q)
  avr_ret_VW_sl.append(avr_ret_VW_sl_q)

print(avr_ret_EW)
print(avr_ret_VW)
print(avr_ret_EW_sl)
print(avr_ret_VW_sl)

### Graphs

# Eq. weighted
df = pd.DataFrame(avr_ret_EW)

plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_2], label='Quantile 2')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_3], label='Quantile 3')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_4], label='Quantile 4')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.title('Avr. return eq. weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Value weighted
df = pd.DataFrame(avr_ret_VW)

plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_2], label='Quantile 2')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_3], label='Quantile 3')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_4], label='Quantile 4')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.title('Avr. return value weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Short-long eq. w.
plt.plot(returns_Q9.index[STARTING_MONTH:], avr_ret_EW_sl)
plt.title('Short-long eq. weighted')
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

# Short-long value w.
plt.plot(returns_Q9.index[STARTING_MONTH:], avr_ret_VW_sl)
plt.title('Short-long value weighted')
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

#Report

df = pd.DataFrame(avr_ret_EW)

l1 = [df.iloc[:,QUANTILE_1].mean()]*len(returns_Q9)
l5 = [df.iloc[:,QUANTILE_5].mean()]*len(returns_Q9)

plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.plot(returns_Q9.index[STARTING_MONTH:], l1, label='Q1: Avg. return across months')
plt.plot(returns_Q9.index[STARTING_MONTH:], l5, label='Q5: Avg. return across months')
plt.title('Avg. return eq. weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

df = pd.DataFrame(avr_ret_VW)

l1 = [df.iloc[:,QUANTILE_1].mean()]*len(returns_Q9)
l5 = [df.iloc[:,QUANTILE_5].mean()]*len(returns_Q9)

plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_1], label='Quantile 1')
plt.plot(returns_Q9.index[STARTING_MONTH:], df.iloc[:,QUANTILE_5], label='Quantile 5')
plt.plot(returns_Q9.index[STARTING_MONTH:], l1, label='Q1: Avg. return across months')
plt.plot(returns_Q9.index[STARTING_MONTH:], l5, label='Q5: Avg. return across months')
plt.title('Avg. return value weighted')
plt.legend()
plt.xlabel("Date")
plt.ylabel("Returns")
plt.show()

"""## **Appendix**"""

### Calculate the performance of the EU market overall to have a benchmark for us
  
# Annualized average return = (AAR)
EU_ret = (my_EU_market_returns["returns"] + my_rf["rf"]).mean()*12

# Annualized volatility
EU_vol = (my_EU_market_returns["returns"] + my_rf["rf"]).std()*(math.sqrt(12))

# Sharpe ratio
EU_SR = (EU_ret - my_rf["rf"].mean()*12) / EU_vol

### Make a table with Summary Statistics

# Create data
data = [["Annualized average return", EU_ret], 
        ["Annualized volatility", EU_vol],
        ["Sharpe ratio", EU_SR]]
  
# Define header names
col_names = ["Summary Statistic", "Value"]
  
# Display table
print("EU market portfolio: Summary statistics",'\033[1m')
print(tabulate(data, headers=col_names, tablefmt ="fancy_grid"))

# For illustration purposes in report
print(my_returns_T.iloc[0:5,[8,10,11,12]])